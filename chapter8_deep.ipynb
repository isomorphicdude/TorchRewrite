{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964c45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458a568",
   "metadata": {},
   "source": [
    "Load the data and apply transform as proposed in the paper (although this is for ImageNet instead of CIFAR10): horizontal flip and subtraction of per pixel means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d924d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "bs=64\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                T.RandomHorizontalFlip()\n",
    "            ])\n",
    "\n",
    "cifar10_train = dset.CIFAR10('/data', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=bs, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('/data', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=bs, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('/data', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769f36f",
   "metadata": {},
   "source": [
    "We use the renet structure, composing our network using `resblocks` which skip certain connections to the next layer. The standardstrategy is to define a building block, such as a (Conv2d, ReLU, Conv2d) + skip\n",
    "connection block, and then build the network using the `nn.sequential` api. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c514af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "    Creating one resblock.\n",
    "    With Conv1+bn+ReLU+Conv2+bn+ReLU\n",
    "    '''\n",
    "    def __init__(self, in_chans, out_chans, stride=1, downsample=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_chans,out_chans,kernel_size=3,padding=1,stride=stride)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\\\n",
    "           nonlinearity='relu')\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        # Note the second conv layer does not have stride to avoid reducing too much\n",
    "        self.conv2 = nn.Conv2d(out_chans,out_chans,kernel_size=3,padding=1)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\\\n",
    "           nonlinearity='relu')\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "       \n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        out = self.batch_norm1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            # Downsampling the residual if necessary\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e9c39",
   "metadata": {},
   "source": [
    "The structure is $Conv2d+Subsampling+ResBlock \\ Layers+AvgPool+FullyConnected$. We follow the similar structure proposed in the seminal paper by Kaiming He et al., which adopts conv layers with increasing filters and residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dc893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, block, layers,in_channel=16):\n",
    "        '''\n",
    "        Initialize the NetResDeep network.\n",
    "\n",
    "        Inputs:\n",
    "        - block: a nn.Module object like a ResBlock \n",
    "        - layers: a list of integers specifying the number of extra resblocks\n",
    "        - in_channel: an integer specifying the first downsampled size\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channel = in_channel\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,self.in_channel,kernel_size=3,padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=in_channel)\n",
    "        \n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2) # Increasing stride but also increasing out channels\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=8)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 10)\n",
    "        #self.fc2 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(self.relu(out))\n",
    "        return out\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        '''\n",
    "        Making ResBlock layers.\n",
    "        Inputs:\n",
    "        - block: a nn.Module object like a ResBlock \n",
    "        - out_channels: an integer of the no. of channels that object should have \n",
    "        - blocks: an integer specifying how many such blocks is needed further\n",
    "                  as the first block is used to make in_chans = out_chans\n",
    "        - stride: an integer specifying stride, default 1\n",
    "    \n",
    "        Returns: nn.sequential object \n",
    "        '''\n",
    "        downsample = None # Default no down sampling\n",
    "        if (stride!=1) or (self.in_channel!=out_channels):\n",
    "            # With downsampling as striding with stride specified\n",
    "            downsample = nn.Sequential(\\\n",
    "                nn.Conv2d(self.in_channel, out_channels,stride=stride,kernel_size=3,padding=1),\\\n",
    "                    nn.BatchNorm2d(out_channels)) \n",
    "        block_layers = []\n",
    "        # The blocks are the residual blocks above and can be any other types of blocks\n",
    "        # Making the first ResBlock that downsample the original input\n",
    "        \n",
    "        block_layers.append(block(self.in_channel, out_channels,stride,downsample))\n",
    "\n",
    "        # Next, set the in_channels to be the same as out_channels for consecutive blocks\n",
    "        self.in_channel = out_channels\n",
    "        # Concantenate the blocks\n",
    "        for i in range(1, blocks):\n",
    "            block_layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*block_layers)\n",
    "\n",
    "\n",
    "def test_NetResDeep():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype = dtype)  # minibatch size 64, image size [3, 32, 32]\n",
    "    model = NetResDeep(ResBlock,[2,2,2])\n",
    "    scores = model(x)\n",
    "    print(x.shape)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_NetResDeep()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe830d",
   "metadata": {},
   "source": [
    "We define two functions for training and accuracy checking. Functions are adopted from Stanford cs231n courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eafcf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Check accuracy of a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: a DataLoader object\n",
    "    - model: A PyTorch Module giving the model to evaluate.\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print(f'{datetime.datetime.now()} Got {num_correct} / {num_samples} correct ({100*acc})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d997e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1,print_every = 100):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7eeba5",
   "metadata": {},
   "source": [
    "Train the model with $2$ extra blocks for each layer using optimizer $Adam$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295edc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0, loss = 2.3735\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:32:59.944778 Got 105 / 1000 correct (10.5)\n",
      "\n",
      "Epoch 0 Iteration 100, loss = 1.8104\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:04.234409 Got 391 / 1000 correct (39.1)\n",
      "\n",
      "Epoch 0 Iteration 200, loss = 1.6882\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:08.926244 Got 412 / 1000 correct (41.199999999999996)\n",
      "\n",
      "Epoch 0 Iteration 300, loss = 1.2090\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:15.149018 Got 363 / 1000 correct (36.3)\n",
      "\n",
      "Epoch 0 Iteration 400, loss = 1.2791\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:20.753635 Got 465 / 1000 correct (46.5)\n",
      "\n",
      "Epoch 0 Iteration 500, loss = 1.3510\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:26.318245 Got 548 / 1000 correct (54.800000000000004)\n",
      "\n",
      "Epoch 0 Iteration 600, loss = 1.0987\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:32.172055 Got 497 / 1000 correct (49.7)\n",
      "\n",
      "Epoch 0 Iteration 700, loss = 1.2117\n",
      "Checking accuracy on validation set\n",
      "2021-12-18 22:33:38.491409 Got 543 / 1000 correct (54.300000000000004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(ResBlock,[2,2,2])\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "train(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'NetResDeep.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
